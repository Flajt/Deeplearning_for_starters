Convelutional Neuronale Networks:

convelution: multiply weights with a part (example 3x3 field of numbers from a 18x18 field of numbers)
new matrix will be created
these matrix will be used as input for the next layers and the process repeates

(example for an image) a matrix contains the shape of the figure part we want to detect in numbers (let`s say an L)
see here for our example:
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|0|0|0|0|0|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|20|20|20|20|20|0|0|0|0|0|
0|0|0|0|0|0|0|20|20|20|20|20|20|20|0|0|0|0|0|
(each |symbole between these| is a row (spalte))
and there you can see the 20s forming an L if we use this
on another matrix that represents an T, the values in the middle (|) of the L the tree (hope you now what I mean)
will be multiplied in a matrix operation but the (_) part of the L not (|_ see the L?) and the rest will not get bigger

Pooling:
max pooling is the most often used type of pooling
searching the max value out of a given window to reduce the matrix size
example (max pooling)
...x|(1|2 )======>|xx|26| think that it goes 4 columns in each row to the left + these you can see
...x|(4|26)=====> |xx|xx|
...x|1|0
...x|x|x
...x|x|x
...x|x|x

Normalization:
example the ReLu function to set down all negativ numbers to 0.

Regularization:
example Droput used to turn neurons on and of randomly so the network is forced
to learn new path the data can flow. To prevent Overfitting(in this case I think is that the net is more then
perfect trained on the data the it became but it can`t predict some "unclean"/not generalised data)
So better generalsation is possible.

propability conversion:
example Softmax function. Input (example) some letters (see matrix for L) and get a propabiltie form it.
Most often used with an function to find the highest number (in our case propability) example: the argmax function

gradient descent:
example(very popular) backpropagation compute error from the prediction value and the real value.
Then use the partial derivative with respect our weights and this returns a gradient value, with this the weight values
will be updated recurrsevly (rückwärts).


when to use it:
examples:
image recognition/generation, robot learning, object detection, gasp learning
when you have got spacial data that is 2 or 3D, (spacial means that the position of the data matters; like text or sound)

-----------------------------------------------------------
Notes(general):
dense_layer=combining all learnings together; often used to get propability values (with a function like "softmax"); the dense layer compute the full weight matrix and the full weight matrix
border mode=two usage modes: Full="should the filter move over the input (there he will find zeros for no input)"; valid="returns a smaller input because it only computes were the filter and the
input overlap"

Working on:
Watch Sirajs Videos:
Python for data science, Intro to tensorflow, Into to deeplearning, The Math of intelligence
Read the book: http://www.deeplearningbook.org/
